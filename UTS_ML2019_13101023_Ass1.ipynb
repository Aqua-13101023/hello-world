{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UTS_ML2019_13101023_Ass1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aqua-13101023/hello-world/blob/master/UTS_ML2019_13101023_Ass1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-hNfcP9h1K0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1. Title and the details of the article"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pR90xtUBMyjm",
        "colab_type": "text"
      },
      "source": [
        "# 1. Title and the details of the article\n",
        "\n",
        "Support-vector networks (Cortes & Vapnik 1995)\n",
        "Author: \n",
        "Corinna Cortes \n",
        "Vladimir Vapnik\n",
        "From: AT&T Bell Labs., Holmdel, NJ07733,USA\n",
        "Specific details: Cortes, C. & Vapnik, V. 1995, 'Support-vector networks', Machine Learning, vol. 20, no. 3, pp. 273–97.\n",
        "\n",
        "The link of the github:\n",
        "https://colab.research.google.com/drive/1182DcRPwm4QD-qtEnEOWxWSStZPYE0Bg?authuser=1#scrollTo=M-hNfcP9h1K0&uniqifier=1\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNq8V6cpkdXN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 2. Introduction "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geuRMTeiNEKr",
        "colab_type": "text"
      },
      "source": [
        "# 2. Introduction \n",
        "The first algorithm in the field of pattern recognition is Fisher in the over 60 years before. At that time, he figured out the model with two groups of normal distribution, he used linear functions to estimate the parameters in the decision function. \n",
        "\n",
        "This article reviews the methodology of optima; hyperplanes in order to make a separation of the training data without any error. Thus, some basic concepts in the following paragraph. For instance, the soft margin hyperplane allows us to analyse the treatment in the terms of errors(Burges & Scholkopf n.d.). \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvLXNwNZRhoo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkmvPeo0RlA4",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZycunPpNQc6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## 2.1 Optimal Hyperplanes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQI6yyaTQHtz",
        "colab_type": "text"
      },
      "source": [
        "## 2.1 Optimal Hyperplanes\n",
        "\n",
        "This figure shows the information of classification which contains unknown pattern with support-vector network. From the very beginning, there is a group of input vector, and the method is to divide them into a number of packages which contain a small amount of training parameter in each package. In this stage, there are two possible results to estimate the parameters. The first one is each package of the selected data is not possible to separate data with hyperplane if the full series data cannot be separated, and the other one is the data optimal hyperplane if the separating data is possible. From the figure, the input vector space is put into support vectors which result in non-linear method. In addition, the linear function’s value comes to the classifier. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMtzD8-oQRUw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## 2.2 The soft Margin Hyperplane"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7npfvyOQXuB",
        "colab_type": "text"
      },
      "source": [
        "## 2.2 The soft Margin Hyperplane\n",
        "\n",
        "This section introduces the case in which it is impossible to separate the training data from each other without any error. The study is able to divide the training set into two groups, one is training set can be separated without any error, and another one is the data set can build into optimal separating hyperplane. To calculate the values of the function, we need to minimize each factor in the function, and the rest elements with max values of margins. We called this method the soft margin hyperplane."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6nP4h72Q2M5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## 2.3  The method of convolution of the Dot-Product in Feature Space"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MqkF6WCbjLB",
        "colab_type": "text"
      },
      "source": [
        "## 2.3  The method of convolution of the Dot-Product in Feature Space\n",
        "\n",
        "The previous three algorithms, namely, optimal hyperplanes, the soft margin hyperplane and the method of convolution of the Dot-Product in the feature space, help to build hyperplanes in the view of input space. The first stage is to transfer the n kinds of dimensional vectors into N kinds of dimensional featured vector groups, and the next stage is to classify unknown vectors into the function. However, this kind of dot-product in the view of the feature vectors to construct the input space."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmrfyhvibm7s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 3. Content "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ircbj6aWbq-Z",
        "colab_type": "text"
      },
      "source": [
        "# 3. Content\n",
        "In this part, we will describe the main challenge or problem it attempted to resolve in this paper. \n",
        "\n",
        "## 3.1 Main target of the paper\n",
        "\n",
        "The support vector machine or the support vector network are well known as the good methodology to estimate or compute the pattern recognition problems. To be specified, the support vector network is regarded as a new method for machine learning to solve two-group classification problems. In this paper, the researchers tried to map non-linear input vectors into high level of dimension. In addition, the high dimension feature space constructed a linear decision surface. This paper extends the way of training data from restricted separated training data to non-separable type. The principle of the support vector network is that the data can be separated without human error. \n",
        "\n",
        "## 3.2 The Challenges in the paper \n",
        "During the process of study, there are two challenges need to be fixed, one is the conceptual problem, which is how to figure out a series of separating hyperplane which can separate the data in the later process. Furthermore, another problem is how to treat the high level of dimension in the feature space.\n",
        "\n",
        "## 3.3 The main data analytics methods in the paper \n",
        "\n",
        "### 3.3.1 Building the decision rules \n",
        "In this stage, it is efficient to construct the build a decision rules based on support vector networks. Firstly, the optimization issues should be solved in order to build decision rules by the method of support vector networks. To be specified, the answer is to solve the optimization problems about the training data which contains the support vectors. Furthermore, the optimal decision result is different from each other.\n",
        "### 3.3.2 A universal machine \n",
        "In the previous part, we talked the dot-product method is able to implement different kinds of support vector networks. In this part, we want to use the dot-product method to verify that the support vector network machine is a kind of universal machine. The support vector networks of the function contains a knowledge about building a special convolution function. Thus, the method of supporting vector networks are a kind of general kinds of learning machines which is able to change the decision function by changing the format of the do-product. \n",
        "### 3.3.3 Control of generating \n",
        "In this part, the writer discussed the relationship between controlling the generating function of a learning machine and the support vector networks. there are two important factors which are able to control the capacity of generalization of learning machine, one is the rate of training data, another one is VC dimension. VC dimension is the factor which created by Vapnik and this factor measured the capacity of learning machine. In the function of trade-off, the probabilities of test error are always no more than the value of frequency of training error and the values of confidence level(Peerbhai 2011). \n",
        "\n",
        "If the VC-dimension comes to a small number, the value of the frequency training error comes to a larger one. This is to say, in the support vector network machines, we can control the balance between frequency of error parameters and surface of the decision rule. Thus, it is quite clear that the support vector network function is able to control the VC dimension factors and frequency of errors. \n",
        "\n",
        "# 4.\tThe experimental results \n",
        "The study conducts two ways to show the function of support vector network machine. One is experiments in the plane and another one about the digit recognition. \n",
        "\n",
        "## 4.1 Plane experiments \n",
        "In this part, we build an artificial series of patterns by using dot-products with 2 dimensions. In the following figures, the 2 dimensions are white and black to demonstrate the power of algorithm. The support patterns represented with double circle and the errors is the cross. The solutions of this is there is no degree polynomials to cause less errors. \n",
        "\n",
        "## 4.2 Digit recognition \n",
        "Another experiment is about the digit recognition. There are two different database which we can use to have an experiments on construction of support vector network machine, one is the small data base and another is big one. In this experiment, there are ten groups are contained for each one class. For the maximum output of classification of the unknown patterns groups is finished during the whole process. \n",
        "\n",
        "The experiment one, the study used the US postal services database which recorded actual mail in the US. The classifier of data, like human performance error rate, decision tree CART error rate, layer neural network error rate and the special architecture error rate, is collected. In this experiments, we built polynomial factors based on dot-product, and the input dimension is 256, and the training data set cannot be separated and no-linear. In this experiment, the bound of generating capacity holds at the time the number of support vector changes the mean value of this number. The result of this experiment is that the error probability of the single classifier is no more than 3%. Another experiment is to compare the test error among the different tests, namely, linear classifier, SVN and so on. \n",
        "\n",
        "This paper discussed about the support vector network machine which is regarded as a kind of new learning machine and solve the two-group classification issues. In addition, the support vectors contain three ideas as following: \n",
        "\tOptimal hyperplanes\n",
        "The solution technique is able to extend the solution vector on the support vector\n",
        "\tConvolution of dot-product\n",
        "This extends the experiment idea from linear problems to non-linear problems\n",
        "\tSoft margins\n",
        "The notion for soft margins is not like other algorithms, it encourages training set with errors.\n",
        "This study also compares performance of other typical algorithms, and the results show that this new method perform well in the comparison view. In addition, the support vector network machines are very powerful and a kind of universal method of learning machine. \n",
        "\n",
        "# 5.\tInnovation \n",
        "\n",
        "In this part, we will talk about the innovation of this paper or the “novelty” of the paper. \n",
        "In this article, the study established a new sort of machine learning, namely, support-vector network. The support-vector network can be explained that the input vectors are mapped into high level of dimension feature space with unrestricted non-linear mapping method. In this feature space, a kind of linear decision making surface is built by special properties, and this will ensure the capacity of generating support vector network(Bengio 2009).  \n",
        "\n",
        "Comparing with other similar methodology, this paper is the first paper to extend the experimental range from linear to non-linear with separating error test data. Other studies (Learning & Sciences 1996) compares support vector method in this paper and the classical radial function which comes from Massachiusetts Institute of Technology with handwritten digits. The support vector method yielded the lower level of test error than other method. \n",
        "\n",
        "Comparing with neural networks, the support vector networks seem to have more advantages(Dietrich, Opper & Sompolinsky n.d.1999). One of the easiest method of the network is the perceptron. But it has limited power, since the method uses the classification by simple linear relationship between the hyperplanes(Salve & Jondhale 2010). However, the new type of supported vector uses convex way to generate training data. Thus, the supported vectors might not have strong tender to analysis(Getzmann, Lang & Spremann 2010). \n",
        "\n",
        "# 6.\tTechnical quality \n",
        "The researcher found that the classification speeds of the methods in this paper are slower than the counterparties of the neural networks(Burges & Scholkopf n.d.). The quality of the paper is well organized, and the references comes to more than 20 papers and the readers will be convinced about the process. And the methodology in totally new in this paper, in the later research, this is widely used in many fields like genes and financial institutions(Cagas & Bush 1996).\n",
        "\n",
        "# 7.\tApplication and X-factor \n",
        "This paper is a very good material to have the basic knowledge about how to analysis the supported vectors network in a mathematics way. The mathematics process in this report is quite clear. In addition, the logical of this paper is quite clear that the data is divided into two groups, namely small database and large database, and make a comparison about the different situations. \n",
        "\n",
        "# 8.\tthe quality of the presentation \n",
        "this paper starts from the introduction of the history to analysis the algorithm of the pattern recognition. And then have a simple review of the related concepts like optimal hyperplanes, soft margin hyperplane and the dot-product convolution space method. Then two kinds of database are used to have experiments on US postal services and NIST database. At the end of the paper, the previous classic learning machines are compared. \n",
        " \n",
        "\n",
        "# Reference \n",
        "\n",
        "Bengio, Y. 2009, Learning deep architectures for AI, Foundations and Trends in Machine Learning, vol. 2.\n",
        "\n",
        "Burges, C.J.C. & Scholkopf, B. n.d., Improving the Accuracy and Speed of Support Vector Machines I :\n",
        "\n",
        "Cagas, P. & Bush, C.A. 1992, 'Conformations of type 1 and type 2 oligosaccharides from ovarian cyst glycoprotein by nuclear overhauser effect spectroscopy and T1 simulations', Biopolymers, vol. 32, no. 3, pp. 277–92.\n",
        "\n",
        "Cortes, C. & Vapnik, V. 1995, 'Support-vector networks', Machine Learning, vol. 20, no. 3, pp. 273–97.\n",
        "\n",
        "Dietrich, R., Opper, M. & Sompolinsky, H. n.d., Statistical Mechanics of Support Vector Networks, no. 3, pp. 1–4.\n",
        "\n",
        "Getzmann, A., Lang, S. & Spremann, K. 2010, 'Determinants of The Target Capital Structure and Adjustment Speed – Evidence from Asian Capital Markets', Asian Finance Symposium, no. March, pp. 1–31.\n",
        "\n",
        "Learning, C. & Sciences, C. 1996, Comparing Support Vector Machines with Gaussian Kernels to Radial Basis Function Classi ers, no. 1599.\n",
        "\n",
        "Peerbhai, F. 2011, the International Capital Asset Pricing Model : Empirical Evidence for South Africa.\n",
        "\n",
        "Salve, S.G. & Jondhale, K.C. 2010, 'Shape matching and object recognition using shape contexts', Proceedings - 2010 3rd IEEE International Conference on Computer Science and Information Technology, ICCSIT 2010, vol. 9, no. 24, pp. 471–4.\n",
        "\n",
        "\n"
      ]
    }
  ]
}